{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2020 NVIDIA\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Converstion to NVIDIA TensorRT & Inference\n",
    "\n",
    "Walthrough a generic pipeline for:\n",
    "- Converting a Pytorch network to TensorRT (via ONNX)\n",
    "- With and without dynamic batch\n",
    "- Steps for Running inference using a TensorRT engine in Python\n",
    "\n",
    "#### Environment\n",
    "All steps executed using **NGC Pytorch Docker (v 20.06)**\n",
    "* [GPU Dashboards](https://medium.com/rapids-ai/gpu-dashboards-in-jupyter-lab-757b17aae1d5) installed using\n",
    "```\n",
    "pip install jupyterlab-nvdashboard\n",
    "jupyter labextension install jupyterlab-nvdashboard\n",
    "```\n",
    "* [Netron](https://github.com/lutzroeder/netron) for network visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "* [NIH ChestXray 14 dataset](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)\n",
    "* 112,120 frontal view chest xrays from 30,805 unique subjects\n",
    "* X-ray images are available as bitmaps extracted from the DICOM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filname):\n",
    "    # images are 48x48 = 2304 size vectors\n",
    "    # N = 35887\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    for line in open(filname):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            Y.append(int(row[0]))\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y) # scaling is already done here\n",
    "    X=X.reshape(35887,1,48,48)\n",
    "    return X,Y\n",
    "\n",
    "def get_loader(X,Y,bz):\n",
    "    tensor_X = torch.stack([torch.from_numpy(np.array(i)) for i in X])\n",
    "    tensor_y = torch.stack([torch.from_numpy(np.array(i)) for i in Y])\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(tensor_X, tensor_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=bz)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 1, 48, 48) (35887,)\n"
     ]
    }
   ],
   "source": [
    "bs = 8 \n",
    "X,Y = getData('./NB_images/fer2013.csv')\n",
    "print(X.shape, Y.shape)\n",
    "train_loader = get_loader(X,Y,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([4953,  547, 5121, 8989, 6077, 4002, 6198]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "rn=random.randint(0,len(X)-1)\n",
    "\n",
    "num2label={0:'Anger', 1:'Disgust',2:'Fear', 3:'Happy',4: 'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "plt.imshow(np.squeeze(X[rn]), cmap='gray')\n",
    "plt.title(num2label[Y[rn]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU device  cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "num_classes = 7\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) #44x(48-5+1)(48-5+1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
    "        self.fc1 = nn.Linear(24 * 9 * 9, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "                num_features *=s\n",
    "        return num_features\n",
    "\n",
    "use_gpu=True\n",
    "if use_gpu:  \n",
    "    device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\") # train on gpu number 0 \n",
    "    print(\"using GPU device \", device )\n",
    "\n",
    "\n",
    "# construct the model and move to GPU if using GPU to train\n",
    "net=Net()\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(device)\n",
    "    net.cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader,net, optimizer,criterion, epochs):   \n",
    "    for e in range(epochs):\n",
    "        running_loss =0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        collect_normalizer=0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #print(i, len(data), data[0]['data'].size(), data[0]['label'].size())\n",
    "            #print(i, data[0]['data'].permute(0, 3, 1, 2).size(), data[0]['label'].size())\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].squeeze().type(torch.LongTensor).to(device)\n",
    "            #print(inputs.size(), labels.size())\n",
    "            # zero the parameter gradients\n",
    "            collect_normalizer += len(data[0])\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            pred,predicted =torch.max(outputs.data,1)\n",
    "            pred_list=predicted.data.tolist()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #acc=round(100 * correct / len(data[0][\"data\"]),4)\n",
    "            #print(outputs.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            #print(\"loss is\",loss.item())\n",
    "            running_loss += loss.item()\n",
    "            #print(collect_normalizer)\n",
    "        print('Epoch [{}/{}],  ACC {:.4f}'.format(e + 1, epochs, correct / collect_normalizer ))\n",
    "\n",
    "    return net, optimizer, train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25],  ACC 0.9112\n",
      "Epoch [2/25],  ACC 0.9201\n",
      "Epoch [3/25],  ACC 0.9204\n",
      "Epoch [4/25],  ACC 0.9310\n",
      "Epoch [5/25],  ACC 0.9339\n",
      "Epoch [6/25],  ACC 0.9373\n",
      "Epoch [7/25],  ACC 0.9405\n",
      "Epoch [8/25],  ACC 0.9404\n",
      "Epoch [9/25],  ACC 0.9417\n",
      "Epoch [10/25],  ACC 0.9433\n",
      "Epoch [11/25],  ACC 0.9479\n",
      "Epoch [12/25],  ACC 0.9520\n",
      "Epoch [13/25],  ACC 0.9488\n",
      "Epoch [14/25],  ACC 0.9533\n",
      "Epoch [15/25],  ACC 0.9555\n",
      "Epoch [16/25],  ACC 0.9605\n",
      "Epoch [17/25],  ACC 0.9565\n",
      "Epoch [18/25],  ACC 0.9599\n",
      "Epoch [19/25],  ACC 0.9648\n",
      "Epoch [20/25],  ACC 0.9677\n",
      "Epoch [21/25],  ACC 0.9669\n",
      "Epoch [22/25],  ACC 0.9652\n",
      "Epoch [23/25],  ACC 0.9689\n",
      "Epoch [24/25],  ACC 0.9645\n",
      "Epoch [25/25],  ACC 0.9661\n"
     ]
    }
   ],
   "source": [
    "epochs=25\n",
    "net, optimizer, train_loader =train_fn(train_loader, net, optimizer,criterion, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save({\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "\n",
    "            }, './ckpt/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1944, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "loaded_model = Net()\n",
    "optimizer = optim.SGD(loaded_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load('./ckpt/model.pt')\n",
    "loaded_model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "input_names=['input']\n",
    "output_names=['output']\n",
    "dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "\n",
    "# A dummy input is needed to generate the ONNX model\n",
    "# Using NCHW format\n",
    "dummy_input = torch.randn(8, 1, 48,48) \n",
    "\n",
    "# No dynamic batch dimension\n",
    "torch.onnx.export(loaded_model, dummy_input, \"./ckpt/model_weights.onnx\", verbose=False, \n",
    "                  input_names=input_names, output_names=output_names)\n",
    "\n",
    "# With dynamic batch dimension\n",
    "torch.onnx.export(loaded_model, dummy_input, \"./ckpt/model_weights_dynamicbatch.onnx\", verbose=False, \n",
    "                  input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "#Load the ONNX Model\n",
    "model_onnx = onnx.load(\"./ckpt/model_weights_dynamicbatch.onnx\")\n",
    "\n",
    "# # Check that the IR is well formed\n",
    "onnx.checker.check_model(model_onnx)\n",
    "\n",
    "# # Print a human readable representation of the graph\n",
    "# onnx.helper.printable_graph(model_onnx.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1.1M Aug 17 10:07 ./ckpt/model_weights.onnx\n",
      "-rw-r--r-- 1 root root 1.1M Aug 17 10:07 ./ckpt/model_weights_dynamicbatch.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./ckpt/*.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the ONNX network in Netron (at http://localhost:8080/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 48, 48]) tensor([0, 0, 2, 4, 6, 2, 4, 3])\n",
      "torch.Size([8, 1, 48, 48]) tensor([3, 2, 0, 6, 6, 6, 3, 5])\n",
      "torch.Size([8, 1, 48, 48]) tensor([3, 2, 6, 4, 4, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,data in enumerate(train_loader):\n",
    "    if i<=2:\n",
    "        im=data[0].to(device)\n",
    "        lb=data[1].squeeze().type(torch.LongTensor)\n",
    "        print(im.size(),lb)\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 48, 48)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 5, 0, 3, 1, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "loaded_model.cuda()\n",
    "with torch.no_grad():\n",
    "    outputs_pyt = loaded_model(im)\n",
    "    outputs_pyt=torch.argmax(outputs_pyt,dim=0)\n",
    "    print(outputs_pyt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 48, 48) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 5, 5, 0, 3, 1, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run ONNX inference\n",
    "import onnxruntime as ort\n",
    "\n",
    "#ort_session = ort.InferenceSession('./ckpt/model_weights.onnx')\n",
    "ort_session = ort.InferenceSession('./ckpt/model_weights_dynamicbatch.onnx')\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy().astype('float32') \n",
    "# adding .astype('float32') refer to this issue http://www.xavierdupre.fr/app/onnxruntime/helpsphinx/auto_examples/plot_common_errors.html\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "print(to_numpy(im).shape, type(to_numpy(im)))\n",
    "\n",
    "ort_outs = ort_session.run(None, {'input': to_numpy(im)})\n",
    "\n",
    "img_out_y = ort_outs[0] # 'input is the layer name specified earlier'\n",
    "np.argmax(img_out_y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "**Network Definition**: interface provides methods for the application to specify the definition of a network.\n",
    "\n",
    "**Builder Configuration**: interface specifies details for creating an engine.\n",
    "\n",
    "**Builder**: allows the creation of an optimized engine from a network definition and a builder configuration.\n",
    "\n",
    "**Engine**: allows the application to execute inference. It supports synchronous and asynchronous execution, profiling, and enumeration and querying of the bindings for the engine inputs and outputs.\n",
    "\n",
    "An **Optimization profile** specifies constraints on dynamic dimensions. It describes a range of dimensions for each network input and the dimensions that the auto-tuner should use for optimization. When using runtime dimensions, you must create at least one optimization profile at build time. Two profiles can specify disjoint or overlapping ranges.\n",
    "\n",
    "For example, one profile might specify a minimum size of [3,100,200], a maximum size of [3,200,300], and optimization dimensions of [3,150,250] while another profile might specify min, max and optimization dimensions of [3,200,100], [3,300,400], and [3,250,250].\n",
    "\n",
    "\n",
    "\n",
    "> Note, if your TensorRT engine has fixed batch size and input shapes, then you **do not** need to worry about optimization profile(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "*Implicit batch* networks were previously the standard up until TensorRT 6. They supported variable batch size through the use of the builder.maxBatchSize attribute, but do not support variable shapes for any of the other dimensions.\n",
    "\n",
    "*Explicit Batch* networks introduced a few changes to the TensorRT API. \n",
    "First, inference is instead performed using execute_v2(bindings) and execute_async_v2(bindings, stream) , which no longer require a batch_size argument since it is taken from the context binding dimensions explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample commands\n",
    "\n",
    "Simple network with no dynamic batch dimension\n",
    "```\n",
    "trtexec --explicitBatch \\\n",
    "            --onnx=model_weights.onnx \\\n",
    "            --saveEngine=trt_export.engine \n",
    "```\n",
    "\n",
    "With dynamic batch\n",
    "``` \n",
    "trtexec --explicitBatch \\\n",
    "            --onnx=model_weights_dynamicbatch.onnx \\\n",
    "            --minShapes=input:1x1x48x48 \\\n",
    "            --optShapes=input:4x1x48x48 \\\n",
    "            --maxShapes=input:8x1x48x48 \\\n",
    "            --shapes=input:4x1x48x48 \\\n",
    "            --saveEngine=trt_export_dynamicbatch.engine \n",
    "```\n",
    "\n",
    "For generating and engine in FP16\n",
    "``` \n",
    "trtexec --explicitBatch \\\n",
    "            --onnx=model_weights_dynamicbatch.onnx \\\n",
    "            --minShapes=input:1x1x48x48 \\\n",
    "            --optShapes=input:4x1x48x48 \\\n",
    "            --maxShapes=input:8x1x48x48 \\\n",
    "            --shapes=input:4x1x48x48 \\\n",
    "            --saveEngine=trt_export_dynamicbatch_fp16.engine \\\n",
    "            --fp16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trtexec --explicitBatch --onnx=./ckpt/model_weights_dynamicbatch.onnx --minShapes=input:1x1x48x48 --optShapes=input:8x1x48x48 --maxShapes=input:96x1x48x48 --shapes=input:80x3x224x224 --saveEngine=./ckpt/trt_export_dynamicbatch_own_model.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = 'trtexec --explicitBatch --onnx=./ckpt/model_weights_dynamicbatch.onnx --minShapes=input:1x1x48x48 --optShapes=input:8x1x48x48 --maxShapes=input:96x1x48x48 --shapes=input:80x3x224x224 --saveEngine=./ckpt/trt_export_dynamicbatch_own_model.engine'\n",
    "print(cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python onnx_to_tensorrt7.py --model=./ckpt/model_weights.onnx --output=model_trt_weights.engine\n"
     ]
    }
   ],
   "source": [
    "cmd = 'python onnx_to_tensorrt7.py --model=./ckpt/model_weights.onnx --output=model_trt_weights.engine'\n",
    "print(cmd)\n",
    "# os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engine(filename):\n",
    "    # Load serialized engine file into memory\n",
    "    with open(filename, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "def inspect_engine(engine):\n",
    "    profile_meta = {}\n",
    "    num_bindings_per_profile = engine.num_bindings // engine.num_optimization_profiles\n",
    "    for profile_index in range(engine.num_optimization_profiles):\n",
    "        start_binding = profile_index * num_bindings_per_profile\n",
    "        end_binding = start_binding + num_bindings_per_profile\n",
    "        \n",
    "        binding_meta = {}\n",
    "        for binding_index in range(start_binding, end_binding):\n",
    "            key = \"Binding {}\".format(binding_index)\n",
    "            binding_meta[key] = {\n",
    "                \"profile\": profile_index,\n",
    "                \"binding_index\": binding_index,\n",
    "                \"binding_shape\": engine.get_binding_shape(binding_index),\n",
    "                \"binding_dtype\": engine.get_binding_dtype(binding_index),\n",
    "                \"binding_name\": engine.get_binding_name(binding_index),\n",
    "            }\n",
    "\n",
    "            if engine.binding_is_input(binding_index):\n",
    "                binding_meta[key][\"binding_type\"] = \"INPUT\"\n",
    "                binding_meta[key][\"profile_shape\"] = engine.get_profile_shape(profile_index, binding_index)\n",
    "            else:\n",
    "                binding_meta[key][\"binding_type\"] = \"OUTPUT\"\n",
    "\n",
    "        profile_meta[\"Profile {}\".format(profile_index)] = binding_meta\n",
    "\n",
    "    from pprint import pprint\n",
    "    pprint(profile_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Profile 0': {'Binding 0': {'binding_dtype': DataType.FLOAT,\n",
      "                             'binding_index': 0,\n",
      "                             'binding_name': 'input',\n",
      "                             'binding_shape': (-1, 1, 48, 48),\n",
      "                             'binding_type': 'INPUT',\n",
      "                             'profile': 0,\n",
      "                             'profile_shape': [(1, 1, 48, 48),\n",
      "                                               (8, 1, 48, 48),\n",
      "                                               (96, 1, 48, 48)]},\n",
      "               'Binding 1': {'binding_dtype': DataType.FLOAT,\n",
      "                             'binding_index': 1,\n",
      "                             'binding_name': 'output',\n",
      "                             'binding_shape': (-1, 7),\n",
      "                             'binding_type': 'OUTPUT',\n",
      "                             'profile': 0}}}\n"
     ]
    }
   ],
   "source": [
    "engine_path = './ckpt/trt_export_dynamicbatch_own_model.engine'\n",
    "# Load a serialized engine into memory\n",
    "engine = load_engine(engine_path)\n",
    "# View various attributes of engine\n",
    "inspect_engine(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Optimization Profile: 0\n"
     ]
    }
   ],
   "source": [
    "# Create context, this can be re-used\n",
    "context = engine.create_execution_context()\n",
    "# Profile 0 (first profile) is used by default\n",
    "context.active_optimization_profile = 0\n",
    "print(\"Active Optimization Profile: {}\".format(context.active_optimization_profile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binding Indices and Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binding_idxs(engine, profile_index):\n",
    "    # Calculate start/end binding indices for current context's profile\n",
    "    num_bindings_per_profile = engine.num_bindings // engine.num_optimization_profiles\n",
    "    start_binding = profile_index * num_bindings_per_profile\n",
    "    end_binding = start_binding + num_bindings_per_profile\n",
    "\n",
    "    print(\"Engine/Binding Metadata\")\n",
    "    print(\"\\tNumber of optimization profiles: {}\".format(engine.num_optimization_profiles))\n",
    "    print(\"\\tNumber of bindings per profile: {}\".format(num_bindings_per_profile))\n",
    "    print(\"\\tFirst binding for profile {}: {}\".format(profile_index, start_binding))\n",
    "    print(\"\\tLast binding for profile {}: {}\".format(profile_index, end_binding-1))\n",
    "\n",
    "\n",
    "    # Separate input and output binding indices for convenience\n",
    "    input_binding_idxs = []\n",
    "    output_binding_idxs = []\n",
    "    for binding_index in range(start_binding, end_binding):\n",
    "        if engine.binding_is_input(binding_index):\n",
    "            input_binding_idxs.append(binding_index)\n",
    "        else:\n",
    "            output_binding_idxs.append(binding_index)\n",
    "\n",
    "    return input_binding_idxs, output_binding_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine/Binding Metadata\n",
      "\tNumber of optimization profiles: 1\n",
      "\tNumber of bindings per profile: 2\n",
      "\tFirst binding for profile 0: 0\n",
      "\tLast binding for profile 0: 1\n"
     ]
    }
   ],
   "source": [
    "# These binding_idxs can change if either the context or the \n",
    "# active_optimization_profile are changed\n",
    "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, context.active_optimization_profile)\n",
    "input_names = [engine.get_binding_name(binding_idx) for binding_idx in input_binding_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dynamic(shape):\n",
    "    return any(dim is None or dim < 0 for dim in shape)\n",
    "\n",
    "def get_image_inputs(engine, context, input_binding_idxs,seed=42):\n",
    "    # Input data for inference\n",
    "    host_inputs = []\n",
    "    print(\"Generating Random Inputs\")\n",
    "    print(\"\\tUsing random seed: {}\".format(seed))\n",
    "    np.random.seed(seed)\n",
    "    for binding_index in input_binding_idxs:\n",
    "        # If input shape is fixed, we'll just use it\n",
    "        input_shape = context.get_binding_shape(binding_index)\n",
    "        input_name = engine.get_binding_name(binding_index)\n",
    "        print(\"\\tInput [{}] shape: {}\".format(input_name, input_shape))\n",
    "        # If input shape is dynamic, we'll arbitrarily select one of the\n",
    "        # the min/opt/max shapes from our optimization profile\n",
    "        if is_dynamic(input_shape):\n",
    "            profile_index = context.active_optimization_profile\n",
    "            profile_shapes = engine.get_profile_shape(profile_index, binding_index)\n",
    "            print(\"\\tProfile Shapes for [{}]: [kMIN {} | kOPT {} | kMAX {}]\".format(input_name, *profile_shapes))\n",
    "            # 0=min, 1=opt, 2=max, or choose any shape, (min <= shape <= max)\n",
    "            input_shape = profile_shapes[1]\n",
    "            print(\"\\tInput [{}] shape was dynamic, setting inference shape to {}\".format(input_name, input_shape))\n",
    "\n",
    "        host_inputs.append(np.random.random(input_shape).astype(np.float32))\n",
    "\n",
    "    return host_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Random Inputs\n",
      "\tUsing random seed: 42\n",
      "\tInput [input] shape: (-1, 1, 48, 48)\n",
      "\tProfile Shapes for [input]: [kMIN (1, 1, 48, 48) | kOPT (8, 1, 48, 48) | kMAX (96, 1, 48, 48)]\n",
      "\tInput [input] shape was dynamic, setting inference shape to (8, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# Generate random inputs based on profile shapes\n",
    "host_inputs = get_image_inputs(engine, context, input_binding_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Metadata\n",
      "\tNumber of Inputs: 1\n",
      "\tInput Bindings for Profile 0: [0]\n",
      "\tInput names: ['input']\n",
      "\tInput shapes: [(8, 1, 48, 48)]\n"
     ]
    }
   ],
   "source": [
    "# Allocate device memory for inputs. This can be easily re-used if the\n",
    "# input shapes don't change\n",
    "device_inputs = [cuda.mem_alloc(h_input.nbytes) for h_input in host_inputs]\n",
    "\n",
    "# Copy host inputs to device, this needs to be done for each new input\n",
    "for h_input, d_input in zip(host_inputs, device_inputs):\n",
    "    cuda.memcpy_htod(d_input, h_input)\n",
    "\n",
    "print(\"Input Metadata\")\n",
    "print(\"\\tNumber of Inputs: {}\".format(len(input_binding_idxs)))\n",
    "print(\"\\tInput Bindings for Profile {}: {}\".format(context.active_optimization_profile, input_binding_idxs))\n",
    "print(\"\\tInput names: {}\".format(input_names))\n",
    "print(\"\\tInput shapes: {}\".format([inp.shape for inp in host_inputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_binding_shapes(engine, context, host_inputs, input_binding_idxs,\n",
    "                  output_binding_idxs, has_input_shape_changed=False):\n",
    "    # Explicitly set the dynamic input shapes, so the dynamic output\n",
    "    # shapes can be computed internally\n",
    "\n",
    "    for host_input, binding_index in zip(host_inputs, input_binding_idxs):\n",
    "        context.set_binding_shape(binding_index, host_input.shape)\n",
    "\n",
    "    assert(context.all_binding_shapes_specified)\n",
    "\n",
    "    host_outputs = [None] * len(output_binding_idxs)\n",
    "    device_outputs = [None] * len(output_binding_idxs)\n",
    "    for i, binding_index in enumerate(output_binding_idxs):\n",
    "        output_shape = context.get_binding_shape(binding_index)\n",
    "        # print(\"output_shape\", output_shape)\n",
    "        # Allocate buffers to hold output results after copying back to host\n",
    "        host_outputs[i] = np.empty(output_shape, dtype=np.float32)\n",
    "        # Allocate output buffers on device\n",
    "        device_outputs[i] = cuda.mem_alloc(host_outputs[i].nbytes)\n",
    "\n",
    "    return host_outputs, device_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Metadata\n",
      "\tNumber of Outputs: 1\n",
      "\tOutput names: ['output']\n",
      "\tOutput shapes: [(8, 7)]\n",
      "\tOutput Bindings for Profile 0: [1]\n"
     ]
    }
   ],
   "source": [
    "# This needs to be called everytime your input shapes change\n",
    "# If your inputs are always the same shape (same batch size, etc.),\n",
    "# then you will only need to call this once\n",
    "host_outputs, device_outputs = setup_binding_shapes(\n",
    "    engine, context, host_inputs, input_binding_idxs, output_binding_idxs,\n",
    ")\n",
    "output_names = [engine.get_binding_name(binding_idx) for binding_idx in output_binding_idxs]\n",
    "\n",
    "print(\"Output Metadata\")\n",
    "print(\"\\tNumber of Outputs: {}\".format(len(output_binding_idxs)))\n",
    "print(\"\\tOutput names: {}\".format(output_names))\n",
    "print(\"\\tOutput shapes: {}\".format([out.shape for out in host_outputs]))\n",
    "print(\"\\tOutput Bindings for Profile {}: {}\".format(context.active_optimization_profile, output_binding_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TRT inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Outputs Shape: (8, 7)\n"
     ]
    }
   ],
   "source": [
    "# Bindings are a list of device pointers for inputs and outputs\n",
    "bindings = device_inputs + device_outputs\n",
    "\n",
    "# Inference\n",
    "context.execute_v2(bindings)\n",
    "\n",
    "# Copy outputs back to host to view results\n",
    "for h_output, d_output in zip(host_outputs, device_outputs):\n",
    "    cuda.memcpy_dtoh(h_output, d_output)\n",
    "\n",
    "# View outputs\n",
    "print(\"Inference Outputs Shape:\", host_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trt_inference_outputs(context,host_inputs,host_outputs,device_inputs, device_outputs):\n",
    "    if not isinstance(host_inputs, list) and isinstance(host_inputs, np.ndarray): host_inputs = [host_inputs]\n",
    "    else: raise ValueError('host inputs must be list of numpy-nd arrays')\n",
    "            \n",
    "    for h_input, d_input in zip(host_inputs, device_inputs): # Copy new inputs\n",
    "        cuda.memcpy_htod(d_input, h_input)\n",
    "    \n",
    "    bindings = device_inputs + device_outputs\n",
    "    context.execute_v2(bindings)\n",
    "\n",
    "    for h_output, d_output in zip(host_outputs, device_outputs):\n",
    "        cuda.memcpy_dtoh(h_output, d_output)  # Get outputs from device\n",
    "    \n",
    "    return host_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prob2pred(model_output):\n",
    "    print(len(model_output),model_output[0].shape)\n",
    "    \n",
    "    pred=np.argmax(model_output[0],axis=0)\n",
    "    \n",
    "    labels=np.array([num2label[o] for o in pred])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (8, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Surprise', 'Surprise', 'Anger', 'Happy', 'Disgust',\n",
       "       'Fear'], dtype='<U8')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_trt_fp32 = get_trt_inference_outputs(context,to_numpy(im),host_outputs,device_inputs,device_outputs)\n",
    "process_prob2pred(result_trt_fp32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del context\n",
    "del engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'trtexec --explicitBatch --onnx=./ckpt/model_weights_dynamicbatch.onnx --minShapes=input:1x1x48x48 --optShapes=input:8x1x48x48 --maxShapes=input:96x1x48x48 --shapes=input:8x1x48x48 --saveEngine=./ckpt/trt_export_dynamic_ownmodel.engine --fp16'\n",
    "# print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1.2M Aug 17 10:55 ./ckpt/trt_export_dynamicbatch_own_model.engine\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./ckpt/*.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Profile 0': {'Binding 0': {'binding_dtype': DataType.FLOAT,\n",
      "                             'binding_index': 0,\n",
      "                             'binding_name': 'input',\n",
      "                             'binding_shape': (-1, 1, 48, 48),\n",
      "                             'binding_type': 'INPUT',\n",
      "                             'profile': 0,\n",
      "                             'profile_shape': [(1, 1, 48, 48),\n",
      "                                               (8, 1, 48, 48),\n",
      "                                               (96, 1, 48, 48)]},\n",
      "               'Binding 1': {'binding_dtype': DataType.FLOAT,\n",
      "                             'binding_index': 1,\n",
      "                             'binding_name': 'output',\n",
      "                             'binding_shape': (-1, 7),\n",
      "                             'binding_type': 'OUTPUT',\n",
      "                             'profile': 0}}}\n",
      "Engine/Binding Metadata\n",
      "\tNumber of optimization profiles: 1\n",
      "\tNumber of bindings per profile: 2\n",
      "\tFirst binding for profile 0: 0\n",
      "\tLast binding for profile 0: 1\n",
      "Generating Random Inputs\n",
      "\tUsing random seed: 42\n",
      "\tInput [input] shape: (-1, 1, 48, 48)\n",
      "\tProfile Shapes for [input]: [kMIN (1, 1, 48, 48) | kOPT (8, 1, 48, 48) | kMAX (96, 1, 48, 48)]\n",
      "\tInput [input] shape was dynamic, setting inference shape to (8, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "engine = load_engine('./ckpt/trt_export_dynamicbatch_own_model.engine')\n",
    "inspect_engine(engine)\n",
    "\n",
    "context = engine.create_execution_context()\n",
    "context.active_optimization_profile = 0\n",
    "\n",
    "input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, context.active_optimization_profile)\n",
    "\n",
    "host_inputs = get_image_inputs(engine, context, input_binding_idxs)\n",
    "device_inputs = [cuda.mem_alloc(h_input.nbytes) for h_input in host_inputs]\n",
    "\n",
    "for h_input, d_input in zip(host_inputs, device_inputs):\n",
    "    cuda.memcpy_htod(d_input, h_input)\n",
    "\n",
    "# Placeholder for output buffers, will resize as necessary\n",
    "host_outputs, device_outputs = setup_binding_shapes(engine, context, host_inputs,\n",
    "                                                    input_binding_idxs, output_binding_idxs)\n",
    "bindings = device_inputs + device_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (8, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Surprise', 'Surprise', 'Anger', 'Happy', 'Disgust',\n",
       "       'Fear'], dtype='<U8')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_trt_fp16 = get_trt_inference_outputs(context,to_numpy(im),host_outputs,device_inputs,device_outputs)\n",
    "process_prob2pred(result_trt_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling\n",
    "\n",
    "`nsys profile -y 0 -w true -t cudnn,cuda,osrt,nvtx -o Report.qdrep python run_inference.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
